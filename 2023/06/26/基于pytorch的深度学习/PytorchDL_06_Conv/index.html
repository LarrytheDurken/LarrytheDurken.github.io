<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="介绍神经网络中卷积层的概念，包括卷积的填充、步长和池化操作。同时对比卷积和全连接层的区别。">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch深度学习-第六节：卷积神经网络">
<meta property="og:url" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/index.html">
<meta property="og:site_name" content="码林">
<meta property="og:description" content="介绍神经网络中卷积层的概念，包括卷积的填充、步长和池化操作。同时对比卷积和全连接层的区别。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/1.png">
<meta property="og:image" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/2.png">
<meta property="og:image" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/3.png">
<meta property="og:image" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/4.png">
<meta property="og:image" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/5.png">
<meta property="og:image" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/6.png">
<meta property="og:image" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/7.png">
<meta property="article:published_time" content="2023-06-26T13:38:23.000Z">
<meta property="article:modified_time" content="2023-06-26T14:47:48.899Z">
<meta property="article:author" content="刘植林">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/1.png">


<link rel="canonical" href="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/","path":"2023/06/26/基于pytorch的深度学习/PytorchDL_06_Conv/","title":"Pytorch深度学习-第六节：卷积神经网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Pytorch深度学习-第六节：卷积神经网络 | 码林</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">码林</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">刘植林，留植林。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">5</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">16</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.</span> <span class="nav-text">卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.1.</span> <span class="nav-text">二维图像卷积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E4%BD%9C%E7%94%A8"><span class="nav-number">1.2.</span> <span class="nav-text">卷积作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A1%AB%E5%85%85"><span class="nav-number">1.3.</span> <span class="nav-text">填充</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A5%E9%95%BF"><span class="nav-number">1.4.</span> <span class="nav-text">步长</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E7%BB%B4%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.5.</span> <span class="nav-text">三维图像卷积</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">2.1.</span> <span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88%E6%B1%87%E8%81%9A%E5%B1%82%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">池化层（汇聚层）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Lenet%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">卷积神经网络（Lenet）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9B%B8%E6%AF%94%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">3.</span> <span class="nav-text">卷积层相比全连接层的优势</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%BC%98%E5%8A%BF"><span class="nav-number">3.1.</span> <span class="nav-text">参数优势</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E6%80%A7%E4%BC%98%E5%8A%BF"><span class="nav-number">3.2.</span> <span class="nav-text">特性优势</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="刘植林"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">刘植林</p>
  <div class="site-description" itemprop="description">No gain,no pain.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/26/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_06_Conv/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="刘植林">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码林">
      <meta itemprop="description" content="No gain,no pain.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Pytorch深度学习-第六节：卷积神经网络 | 码林">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch深度学习-第六节：卷积神经网络
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-06-26 21:38:23 / 修改时间：22:47:48" itemprop="dateCreated datePublished" datetime="2023-06-26T21:38:23+08:00">2023-06-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">pytorch深度学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <center>介绍神经网络中卷积层的概念，包括卷积的填充、步长和池化操作。同时对比卷积和全连接层的区别。</center>

<span id="more"></span>
<h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><h2 id="二维图像卷积"><a href="#二维图像卷积" class="headerlink" title="二维图像卷积"></a>二维图像卷积</h2><p>卷积是个错误叫法，实际运算其实是互相关运算。<br>在⼆维互相关运算中，卷积窗口从输⼊张量的左上角开始，从左到右、从上到下滑动。当卷积窗口滑动到新⼀个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到⼀个单⼀的标量值，由此我们得出了这⼀位置的输出张量值。如：</p>
<p><div style="width: 300px; margin: auto"><img src="1.png" alt></div></p>
<p>即：</p>
<center>$0 \times 0 + 1 \times 1 + 3 \times 2 + 4 \times 3 = 19$</center>
<center>$1 \times 0 + 2 \times 1 + 4 \times 2 + 5 \times 3 = 25$</center>
<center>$3 \times 0 + 4 \times 1 + 6 \times 2 + 7 \times 3 = 37$</center>
<center>$4 \times 0 + 5 \times 1 + 7 \times 2 + 8 \times 3 = 43$</center>

<h2 id="卷积作用"><a href="#卷积作用" class="headerlink" title="卷积作用"></a>卷积作用</h2><p>卷积可以实现许多内容，比如Sobel算子可以提取图像边缘特征，还有一些平均滤波器可以对图像进行高斯模糊。这里不做具体概述，详情可查询数字图像处理相关知识。</p>
<h2 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h2><p>如上图所示，如果对原图像张量进行卷积，最终结果会小于原张量的原尺寸，体现在图像上就是会丢失图像边框部分的像素。因此，在对图像卷积前，我们可以在输⼊图像的边界填充元素（通常填充元素是0）。如图：</p>
<p><div style="width: 400px; margin: auto"><img src="2.png" alt></div></p>
<p>假设图像有$n_h$行$n_w$列，卷积核有$k_h$行$k_w$列，添加$p_h$行$p_w$列，则输出形状为：</p>
<center>$(n_h-k_h+p_h+1) \times (n_w-k_w+p_w+1)$</center>

<p>卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。</p>
<h2 id="步长"><a href="#步长" class="headerlink" title="步长"></a>步长</h2><p>在前面的例子中，我们默认每次滑动⼀个元素。但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。<br>我们将每次滑动元素的数量称为步幅（stride）。下图就是垂直步幅为3，⽔平步幅为2的⼆维互相关运算。着色部分是输出元素以及⽤于输出计算的输⼊和内核张量元素。</p>
<p><div style="width: 400px; margin: auto"><img src="3.png" alt></div></p>
<p>当垂直步幅为$s_h$、⽔平步幅为$s_w$时，输出形状为:</p>
<center>$[(n_h-k_h+p_h+1)/s_h] \times [(n_w-k_w+p_w+1)/s_w]$</center>

<h2 id="三维图像卷积"><a href="#三维图像卷积" class="headerlink" title="三维图像卷积"></a>三维图像卷积</h2><p>当输入包含多个通道时，需要构造一个与输⼊数据具有相同输入通道数的卷积核，以便与输⼊数据进行互相关运算。<br>输入输⼊有$c_i$个通道，那么卷积核也应有$c_i$个通道。然后以对每个通道输⼊的二维张量和卷积核的二维张量进⾏互相关运算，再对通道求和（将$c_i$的结果相加）得到二维张量。</p>
<p><div style="width: 400px; margin: auto"><img src="4.png" alt></div></p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p><div style="width: 400px; margin: auto"><img src="5.png" alt></div></p>
<p>如图就是一个独立的卷积层，其包含两个神经元（有两个不同的核函数）。注意，卷积层输出的通道数仅取决于神经元个数。<br>不同的神经元可以代表不同的处理，比如第一个神经元的核函数可以代表图像水平边缘，第二个神经元的核函数可以代表图像垂直边缘，通过这两个特性来实现图像的边缘特征提取。</p>
<h2 id="池化层（汇聚层）"><a href="#池化层（汇聚层）" class="headerlink" title="池化层（汇聚层）"></a>池化层（汇聚层）</h2><p>与卷积层类似，池化层运算符由⼀个固定形状的窗⼝组成，该窗口根据其步幅大小在输⼊的所有区域上滑动，为固定形状窗⼝遍历的每个位置计算一个输出。然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为最大池化层（maximum pooling）和平均池化层（average pooling）。</p>
<p><div style="width: 400px; margin: auto"><img src="6.png" alt></div></p>
<p>对于池化层的机制，以最大池化层为例，某种解释是它提取了图像局部特征中最主要的特征（最大值），所以可以提升神经网络的效果。<br>但实际上池化层为何有效并没有十分确切的答案，只能说是基于经验来看它确实有效。</p>
<h2 id="卷积神经网络（Lenet）"><a href="#卷积神经网络（Lenet）" class="headerlink" title="卷积神经网络（Lenet）"></a>卷积神经网络（Lenet）</h2><p>Lenet是最早发布的卷积神经网络之一，目的是识别图像中的手写数字。<br>总体来看，LeNet由两个部分组成：</p>
<ul>
<li>卷积编码器：由两个卷积层组成,负责学习图片空间特征</li>
<li>全连接层密集块：由三个全连接层组成，负责将图片转换为类别空间</li>
</ul>
<p><div style="width: 600px; margin: auto"><img src="7.png" alt></div></p>
<p>每个卷积块中的基本单元是⼀个卷积层、⼀个sigmoid激活函数和平均汇聚层。每个卷积层使⽤$5 \times 5$卷积核和一个sigmoid激活函数。这些层将输⼊映射到多个二维特征输出，通常同时增加通道的数量。第⼀卷积层有6个输出通道，⽽第⼆个卷积层有16个输出通道。每个$2 \times 2$池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量⼤小、通道数、高度、宽度决定。<br>为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本。换言之，我们将这个四维输⼊转换成全连接层所期望的二维输⼊。这⾥的二维表⽰的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表⽰。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。</p>
<h1 id="卷积层相比全连接层的优势"><a href="#卷积层相比全连接层的优势" class="headerlink" title="卷积层相比全连接层的优势"></a>卷积层相比全连接层的优势</h1><h2 id="参数优势"><a href="#参数优势" class="headerlink" title="参数优势"></a>参数优势</h2><p>在输入是一个图像的情况下，如果采用全连接层，那么图像中每一个像素都将需要一个权重参数$w$与其对应，在如今像素值往往能达到千万甚至亿万数量级的情况下，仅仅构建一个单层全连接神经网络也将需要大量的参数开销，这显然很不科学。<br>而卷积层仅需要的参数仅仅是卷积核内的参数，相比全连接层要少很多，这使得我们可以轻而易举的只占用很少量的内存空间就能构建深层神经网络。</p>
<h2 id="特性优势"><a href="#特性优势" class="headerlink" title="特性优势"></a>特性优势</h2><p>同时，卷积操作也满足了平移不变性和局部性两个特点：</p>
<ul>
<li>平移不变性：不论卷积核移动到图像哪一个位置，其参数永远只与卷积核本身相关，与图像位置无关。</li>
<li>局部性：卷积操作只关注图像中某一部分的局部特征。</li>
</ul>
<p>这些是全连接层所不具备的特点，且这两个特点在图像处理中十分有用。比如，我们希望识别处场景中的猫，那么不论猫在哪里神经网络都应该正确判断，而这个猫肯定只占图像中的一小部分，而不是填充满整个画幅。</p>

    </div>

    
    
    

<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">---------------------本文结束---------------------</div>
    
</div>
  
</div>

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i>深度学习</a>
              <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i>pytorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/06/07/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_05_Optimize/" rel="prev" title="Pytorch深度学习-第五节：网络的优化">
                  <i class="fa fa-chevron-left"></i> Pytorch深度学习-第五节：网络的优化
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/06/29/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/DataStructure_01_LinearList/" rel="next" title="数据结构-第一章：线性表">
                  数据结构-第一章：线性表 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-tree"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李林晁</span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"mhchem":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
