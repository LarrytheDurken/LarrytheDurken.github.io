<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="从最简单的单层线性网络介绍神经网络的概念，同时然后再介绍通过激活函数实现更一般的网络，在其中阐述一些基本概念。">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch深度学习-第四节：神经网络">
<meta property="og:url" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/index.html">
<meta property="og:site_name" content="码林">
<meta property="og:description" content="从最简单的单层线性网络介绍神经网络的概念，同时然后再介绍通过激活函数实现更一般的网络，在其中阐述一些基本概念。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/3.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/1.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/2.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/4.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/5.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/6.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/7.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/8.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/9.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/10.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/11.png">
<meta property="og:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/12.png">
<meta property="article:published_time" content="2023-06-03T12:46:23.000Z">
<meta property="article:modified_time" content="2023-06-26T12:26:17.993Z">
<meta property="article:author" content="刘植林">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/3.png">


<link rel="canonical" href="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/","path":"2023/06/03/基于pytorch的深度学习/PytorchDL_04_SimpleNet/","title":"Pytorch深度学习-第四节：神经网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Pytorch深度学习-第四节：神经网络 | 码林</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">码林</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">刘植林，留植林。</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">5</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">17</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">线性神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.</span> <span class="nav-text">神经网络图与神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="nav-number">1.3.</span> <span class="nav-text">解析解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.4.</span> <span class="nav-text">随机梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.5.</span> <span class="nav-text">正向传播与反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88%E7%A8%A0%E5%AF%86%E5%B1%82%EF%BC%89"><span class="nav-number">1.6.</span> <span class="nav-text">全连接层（稠密层）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A2%E9%87%8F%E5%8C%96%E5%8A%A0%E9%80%9F"><span class="nav-number">1.7.</span> <span class="nav-text">矢量化加速</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="nav-number">2.</span> <span class="nav-text">通过激活函数实现非线性变换</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E5%85%A5%E9%9A%90%E8%97%8F%E5%B1%82"><span class="nav-number">2.1.</span> <span class="nav-text">加入隐藏层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E5%85%A5%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">加入激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ReLU%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.1.</span> <span class="nav-text">ReLU函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.2.</span> <span class="nav-text">sigmoid函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tanh%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.3.</span> <span class="nav-text">tanh函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%9F%A9%E9%98%B5%E7%BB%B4%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">神经网络中的矩阵维数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AA%E7%9F%A2%E9%87%8F%E5%8C%96"><span class="nav-number">3.1.</span> <span class="nav-text">未矢量化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A2%E9%87%8F%E5%8C%96"><span class="nav-number">3.2.</span> <span class="nav-text">矢量化</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="刘植林"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">刘植林</p>
  <div class="site-description" itemprop="description">No gain,no pain.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/03/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_04_SimpleNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="刘植林">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码林">
      <meta itemprop="description" content="No gain,no pain.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Pytorch深度学习-第四节：神经网络 | 码林">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch深度学习-第四节：神经网络
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-06-03 20:46:23" itemprop="dateCreated datePublished" datetime="2023-06-03T20:46:23+08:00">2023-06-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-26 20:26:17" itemprop="dateModified" datetime="2023-06-26T20:26:17+08:00">2023-06-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">pytorch深度学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <center>从最简单的单层线性网络介绍神经网络的概念，同时然后再介绍通过激活函数实现更一般的网络，在其中阐述一些基本概念。</center>

<span id="more"></span>
<h1 id="线性神经网络"><a href="#线性神经网络" class="headerlink" title="线性神经网络"></a>线性神经网络</h1><h2 id="神经网络图与神经网络"><a href="#神经网络图与神经网络" class="headerlink" title="神经网络图与神经网络"></a>神经网络图与神经网络</h2><p><div style="width: 300px; margin: auto"><img src="3.png" alt></div></p>
<p>如图是一个简单的神经网络，输入为$x_1, …,x_d$，因此特征维度为$d$，网络的输出为$o_1$。<br>其中，$o_1 = (w_1 x_1+b_1)+…+(w_d x_d +b_d)$<br>对于一个具体的样本$(X, y)$，我们通过上式由输入$X$得到输出$o$，计量预测值$o$与真实值$y$之间的误差，不断寻找能使误差最小的$w$和$b$，即可进行良好的预测。这就是神经网络的基本思想。</p>
<p>神经网络图只显示连接模式，即只显示每个输⼊如何连接到输出，隐去了权重和偏置的值。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数（loss function）能够量化目标的实际值与预测值之间的差距。通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为$0$。<br>在训练模型时，我们希望寻找⼀组参数$(w^∗, b^∗)$，这组参数能最小化在所有训练样本上的总损失。如下式：</p>
<p><div style="width: 400px; margin: auto"><img src="1.png" alt></div></p>
<h2 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h2><p>解可以用一个公式简单地表达出来，这类解叫作解析解，如线性模型就有解析解。但大部分模型并不存在解析解。</p>
<h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。<br>但实际中的执行可能会⾮常慢：因为在每⼀次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取⼀小批样本，这种变体叫做小批量随机梯度下降。</p>
<p><div style="width: 400px; margin: auto"><img src="2.png" alt></div></p>
<p>$B$表示每个小批量中的样本数，也称为批量大小（batch size）。$η$表示学习率（learning rate）。</p>
<p>算法的步骤如下：</p>
<ul>
<li>初始化模型参数的值</li>
<li>从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这⼀步骤</li>
</ul>
<h2 id="正向传播与反向传播"><a href="#正向传播与反向传播" class="headerlink" title="正向传播与反向传播"></a>正向传播与反向传播</h2><p>正向传播就是根据输入得到输出的过程（即：$o_1 = (w_1 x_1+b_1)+…+(w_d x_d +b_d)$），其目的在于通过损失函数得到预测值与真实值的误差，相当于一个评估过程。</p>
<p>反向传播则是根据正向传播得到的预测值，对网络中的各个参数（即：$w$，$b$）进行梯度下降，其目的在于使模型不断接近收敛，相当于一个优化过程。</p>
<h2 id="全连接层（稠密层）"><a href="#全连接层（稠密层）" class="headerlink" title="全连接层（稠密层）"></a>全连接层（稠密层）</h2><p>每个输⼊都与每个输出相连，我们将这种变换称为全连接层（fully-connected layer）或称为稠密层（dense layer）。如图：</p>
<p><div style="width: 300px; margin: auto"><img src="4.png" alt></div></p>
<p>1.4中的图片也是全连接层。</p>
<h2 id="矢量化加速"><a href="#矢量化加速" class="headerlink" title="矢量化加速"></a>矢量化加速</h2><p>在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。为了实现这⼀点，我们需要利用线性代数库对计算进行矢量化，而不是在Python中编写开销⾼昂的for循环。</p>
<h1 id="通过激活函数实现非线性变换"><a href="#通过激活函数实现非线性变换" class="headerlink" title="通过激活函数实现非线性变换"></a>通过激活函数实现非线性变换</h1><p>许多模型并非线性的，如果我们只通过仿射变换实现线性的模型，将会受到很大的限制。因此我们需要在神经网络中加入非线性元素。</p>
<h2 id="加入隐藏层"><a href="#加入隐藏层" class="headerlink" title="加入隐藏层"></a>加入隐藏层</h2><p>我们可以通过在⽹络中加入⼀个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这⼀点，最简单的⽅法是将许多全连接层堆叠在⼀起。每⼀层都输出到上面的层，直到生成最后的输出。我们可以把前$L−1$层看作表⽰，把最后⼀层看作线性预测器。</p>
<p><div style="width: 300px; margin: auto"><img src="5.png" alt></div></p>
<h2 id="加入激活函数"><a href="#加入激活函数" class="headerlink" title="加入激活函数"></a>加入激活函数</h2><p>在仿射变换之后对每个隐藏单元应⽤非线性的<br>激活函数（activation function）σ。激活函数的输出（例如，σ(·)）被称为活性值（activations）。<br>此时，图2.1的关系即为：</p>
<center>$H = \sigma (XW^{(1)}+b^{1})$</center>
<center>$O = XW^{(2)}+b^{2}$</center>

<p>我们可以继续堆叠这样的隐藏层，从⽽产⽣更有表达能⼒的模型。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>最受欢迎的激活函数是修正线性单元（Rectified linear unit，ReLU），因为它实现简单，同时在各种预测任务中表现良好。ReLU提供了⼀种⾮常简单的⾮线性变换。给定元素x，ReLU函数被定义为该元素与0的最⼤值：</p>
<center>$ReLU(x)=max(x,0)$</center>

<p>其函数图像和导数图像分别为：</p>
<p><div style="width: 400px; margin: auto"><img src="6.png" alt></div></p>
<p><div style="width: 400px; margin: auto"><img src="7.png" alt></div></p>
<h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>对于一个定义域在$R$中的输⼊，sigmoid函数将输⼊变换为区间$(0, 1)$上的输出。因此，sigmoid通常称为挤压函数（squashing function）：</p>
<center>$sigmoid(x) = \frac{1}{1+exp(-x)}$</center>

<p>其函数图像和导数图像分别为：</p>
<p><div style="width: 400px; margin: auto"><img src="8.png" alt></div></p>
<p><div style="width: 400px; margin: auto"><img src="9.png" alt></div></p>
<p>sigmoid在隐藏层中已经较少使用，它在⼤部分时候被更简单、更容易训练的ReLU所取代。</p>
<h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>与sigmoid函数类似，tanh(双曲正切)函数也能将其输入压缩转换到区间$(-1, 1)$上。tanh函数的公式如下：</p>
<center>$tanh(x) = \frac{1-exp(-2x)}{1+exp(-2x)}$</center>

<p>其函数图像和导数图像分别为：</p>
<p><div style="width: 400px; margin: auto"><img src="10.png" alt></div></p>
<p><div style="width: 400px; margin: auto"><img src="11.png" alt></div></p>
<h1 id="神经网络中的矩阵维数"><a href="#神经网络中的矩阵维数" class="headerlink" title="神经网络中的矩阵维数"></a>神经网络中的矩阵维数</h1><p>以下图的神经网络为例。</p>
<p><div style="width: 400px; margin: auto"><img src="12.png" alt></div></p>
<h2 id="未矢量化"><a href="#未矢量化" class="headerlink" title="未矢量化"></a>未矢量化</h2><p>这里我们假设样本是一个列向量，即图中输入为$(x_1, x_2)^T$，维数为$(2, 1)$；第一层网络为$(o_1^1, o_2^1, o_3^1)^T$，维数为$(3, 1)$，因此根据矩阵乘法可知，$W^1$应该是一个维数为$(3, 2)$的矩阵，即第一层每个神经元中的$w$应该形如$(w_1, w_2)$。<br>经过推到可以得出，对于$W^l$（$l$指层数），其维度应该为$(n^l, n^{l-1})$（$n$指神经元数量），$w^l_i$的形状为具有$n^{l-1}$的行向量。</p>
<h2 id="矢量化"><a href="#矢量化" class="headerlink" title="矢量化"></a>矢量化</h2><p>假设$X_i$为列向量，则输入矩阵为$(X_1, X_2,…,X_m)$，维数为$(2, m)$；第一层网络为$(O_1^1, O_2^1, O_3^1)^T$，维数为$(3, m)$。可以发现，$W$的维数并没有发生变化，只是神经网络中的特征矩阵发生了维度的叠加。</p>

    </div>

    
    
    

<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">---------------------本文结束---------------------</div>
    
</div>
  
</div>

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i>深度学习</a>
              <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i>pytorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/05/11/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_03_AutoGra/" rel="prev" title="Pytorch深度学习-第三节：自动求导">
                  <i class="fa fa-chevron-left"></i> Pytorch深度学习-第三节：自动求导
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/06/04/%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/PytorchDL_4.1_LoadData(1)/" rel="next" title="Pytorch深度学习-第4.1节：加载数据（1）">
                  Pytorch深度学习-第4.1节：加载数据（1） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-tree"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李林晁</span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"mhchem":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
